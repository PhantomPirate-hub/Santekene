# üÜì OLLAMA - IA LOCALE GRATUITE

## üéØ **POURQUOI OLLAMA ?**

Ollama est une **alternative 100% gratuite** √† OpenAI qui fonctionne **localement** sur votre ordinateur.

### ‚úÖ **Avantages**
- üÜì **Gratuit** : Aucun co√ªt, utilisation illimit√©e
- üîí **Priv√©** : Vos donn√©es restent sur votre ordinateur
- üöÄ **Rapide** : Pas de limite de quota
- üåê **Hors ligne** : Fonctionne sans internet

### ‚ö†Ô∏è **Inconv√©nients**
- L√©g√®rement moins pr√©cis qu'OpenAI GPT-4
- N√©cessite 8GB+ de RAM
- Premier t√©l√©chargement : ~4GB

---

## üöÄ **INSTALLATION OLLAMA (5 MINUTES)**

### üì• **√âtape 1 : T√©l√©charger Ollama**

1. Allez sur : **https://ollama.ai/download**
2. T√©l√©chargez la version **Windows**
3. Installez (double-clic sur le .exe)
4. Suivez l'assistant d'installation

### ü§ñ **√âtape 2 : T√©l√©charger le mod√®le**

Ouvrez un **nouveau terminal** (PowerShell ou CMD) et tapez :

```bash
ollama pull llama3.2
```

‚è≥ **Attendez le t√©l√©chargement** (~4GB, peut prendre 5-10 minutes)

Vous verrez :
```
pulling manifest
pulling 8eeb52dfb3bb... 100%
success
```

### ‚úÖ **√âtape 3 : Tester Ollama**

Dans le m√™me terminal, tapez :

```bash
ollama run llama3.2 "Bonjour, comment vas-tu ?"
```

Si vous voyez une r√©ponse en fran√ßais, **c'est bon !** ‚úÖ

---

## üîß **CONFIGURATION DANS SANTEKENE**

### üìù **√âtape 1 : Modifier le backend IA**

Ouvrez le fichier : `backend-ai/main.py`

**Remplacez tout le contenu** par ce code optimis√© pour Ollama :

```python
from fastapi import FastAPI, HTTPException, Form
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import json
import requests

app = FastAPI(
    title="Sant√© K√®n√® - AI Service (Ollama)",
    description="API IA locale avec Ollama",
    version="0.2.0",
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:3001"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

OLLAMA_API_URL = "http://localhost:11434/api/generate"

@app.get("/", tags=["Health Check"])
async def root():
    return {"message": "Sant√© K√®n√® AI Service (Ollama) is running."}

@app.post("/api/ai/triage", tags=["AI - Ollama"])
async def ai_triage(symptoms: str = Form(...), latitude: float = Form(None), longitude: float = Form(None)):
    if not symptoms:
        raise HTTPException(status_code=400, detail="Les sympt√¥mes sont requis.")
    
    try:
        # Prompt optimis√© pour le triage m√©dical
        prompt = f"""Tu es un assistant m√©dical IA expert. Analyse les sympt√¥mes suivants et r√©ponds UNIQUEMENT au format JSON (sans markdown, juste le JSON brut):

Sympt√¥mes du patient: {symptoms}

R√©ponds avec ce format JSON exact:
{{
  "summary": "R√©sum√© concis des sympt√¥mes en 1-2 phrases",
  "urgency": "faible" ou "mod√©r√©" ou "urgent",
  "specialties": ["Sp√©cialit√©1", "Sp√©cialit√©2"],
  "precautions": ["Pr√©caution 1", "Pr√©caution 2", "Pr√©caution 3"],
  "recommendations": ["Recommandation 1", "Recommandation 2"],
  "consultation_type": "presentiel" ou "en_ligne" ou "urgence",
  "explanation": "Explication d√©taill√©e de l'analyse"
}}

Crit√®res d'urgence:
- "urgent": Sympt√¥mes graves (douleur thoracique, difficult√© respiratoire s√©v√®re, saignement important)
- "mod√©r√©": Sympt√¥mes significatifs (fi√®vre √©lev√©e persistante, douleur intense)
- "faible": Sympt√¥mes l√©gers (rhume, l√©g√®re fatigue)

Sp√©cialit√©s disponibles: M√©decine G√©n√©rale, Cardiologie, Dermatologie, P√©diatrie, Gyn√©cologie, Neurologie, Orthop√©die, ORL, Pneumologie, Gastro-ent√©rologie

Type de consultation:
- "urgence": Cas urgents n√©cessitant intervention imm√©diate
- "presentiel": Cas n√©cessitant un examen physique
- "en_ligne": Cas pouvant √™tre g√©r√©s en t√©l√©consultation

R√©ponds UNIQUEMENT en JSON, rien d'autre."""

        # Appel √† Ollama
        response = requests.post(
            OLLAMA_API_URL,
            json={
                "model": "llama3.2",
                "prompt": prompt,
                "stream": False
            },
            timeout=60
        )
        
        if response.status_code != 200:
            raise HTTPException(status_code=500, detail=f"Erreur Ollama: {response.text}")
        
        result = response.json()
        ai_response = result.get("response", "")
        
        # Nettoyer le r√©sultat
        cleaned_result = ai_response.strip()
        if cleaned_result.startswith("```json"):
            cleaned_result = cleaned_result[7:]
        if cleaned_result.startswith("```"):
            cleaned_result = cleaned_result[3:]
        if cleaned_result.endswith("```"):
            cleaned_result = cleaned_result[:-3]
        cleaned_result = cleaned_result.strip()
        
        # Parser le JSON
        try:
            triage_data = json.loads(cleaned_result)
        except json.JSONDecodeError as e:
            print(f"Erreur JSON: {e}")
            print(f"R√©ponse brute: {ai_response}")
            # Fallback
            triage_data = {
                "summary": "Analyse en cours...",
                "urgency": "mod√©r√©",
                "specialties": ["M√©decine G√©n√©rale"],
                "precautions": ["Consultez un m√©decin"],
                "recommendations": ["Prenez rendez-vous"],
                "consultation_type": "presentiel",
                "explanation": ai_response
            }
        
        # Valider et normaliser
        urgency = triage_data.get("urgency", "mod√©r√©").lower()
        if urgency not in ["faible", "mod√©r√©", "urgent"]:
            urgency = "mod√©r√©"
        
        consultation_type = triage_data.get("consultation_type", "presentiel").lower()
        if consultation_type not in ["presentiel", "en_ligne", "urgence"]:
            consultation_type = "presentiel"
        
        # R√©ponse finale
        response_data = {
            "summary": triage_data.get("summary", "Analyse termin√©e"),
            "urgency": urgency,
            "urgency_label": {
                "faible": "Urgence Faible",
                "mod√©r√©": "Urgence Mod√©r√©e",
                "urgent": "Urgence √âlev√©e"
            }.get(urgency, "Urgence Mod√©r√©e"),
            "urgency_color": {
                "faible": "green",
                "mod√©r√©": "orange",
                "urgent": "red"
            }.get(urgency, "orange"),
            "specialties": triage_data.get("specialties", ["M√©decine G√©n√©rale"]),
            "precautions": triage_data.get("precautions", []),
            "recommendations": triage_data.get("recommendations", []),
            "consultation_type": consultation_type,
            "consultation_type_label": {
                "presentiel": "Consultation en pr√©sentiel recommand√©e",
                "en_ligne": "T√©l√©consultation possible",
                "urgence": "Consultation d'urgence n√©cessaire"
            }.get(consultation_type, "Consultation recommand√©e"),
            "explanation": triage_data.get("explanation", ""),
            "raw_response": ai_response
        }
        
        return JSONResponse(response_data)
        
    except requests.exceptions.ConnectionError:
        raise HTTPException(
            status_code=500, 
            detail="Impossible de se connecter √† Ollama. Assurez-vous qu'Ollama est d√©marr√©."
        )
    except Exception as e:
        print(f"Erreur: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")
```

**Sauvegardez le fichier** (Ctrl+S)

---

### üöÄ **√âtape 2 : Red√©marrer le backend IA**

1. **Arr√™tez** le backend IA actuel (Ctrl+C dans le terminal)
2. **Relancez** :
   ```bash
   cd backend-ai
   venv\Scripts\activate
   uvicorn main:app --reload --port 8000
   ```

Vous devriez voir :
```
INFO:     Uvicorn running on http://127.0.0.1:8000
```

---

### üß™ **√âtape 3 : Tester**

1. **Retournez sur l'application** (http://localhost:3000)
2. **Menu** ‚Üí **IA Clinique**
3. **Tapez** : `"J'ai mal √† la t√™te et de la fi√®vre depuis 2 jours"`
4. **Cliquez** : "Analyser mes sympt√¥mes"
5. ‚è≥ **Attendez** (peut prendre 10-15 secondes la premi√®re fois)
6. ‚úÖ **Les r√©sultats s'affichent !**

---

## üêõ **D√âPANNAGE**

### Probl√®me 1 : "Impossible de se connecter √† Ollama"

**Solution** : D√©marrez Ollama

**Windows** :
- Ollama d√©marre automatiquement apr√®s installation
- Si pas d√©marr√© : Cherchez "Ollama" dans le menu D√©marrer et lancez-le
- Ou dans le terminal : `ollama serve`

### Probl√®me 2 : "Model llama3.2 not found"

**Solution** :
```bash
ollama pull llama3.2
```

### Probl√®me 3 : R√©ponses lentes

**C'est normal** la premi√®re fois ! Ollama charge le mod√®le en m√©moire.
- Premi√®re requ√™te : 10-20 secondes
- Requ√™tes suivantes : 2-5 secondes

### Probl√®me 4 : "Out of memory"

**Solution** : Utilisez un mod√®le plus petit
```bash
ollama pull llama3.2:1b
```
Puis dans `main.py`, changez `"model": "llama3.2"` en `"model": "llama3.2:1b"`

---

## üìä **COMPARAISON OLLAMA vs OPENAI**

| Crit√®re | Ollama | OpenAI |
|---------|--------|--------|
| **Prix** | Gratuit ‚úÖ | Payant üí∞ |
| **Vitesse** | 2-5 sec | 1-2 sec |
| **Pr√©cision m√©dicale** | 7/10 | 9/10 |
| **Installation** | Locale | API Cloud |
| **Quota** | Illimit√© ‚úÖ | Limit√© |
| **Vie priv√©e** | 100% priv√© ‚úÖ | Cloud |
| **RAM requise** | 8GB+ | 0 |

---

## ‚úÖ **CHECKLIST**

- [ ] Ollama t√©l√©charg√© et install√©
- [ ] Mod√®le `llama3.2` t√©l√©charg√© (`ollama pull llama3.2`)
- [ ] Test `ollama run llama3.2 "test"` fonctionne
- [ ] Fichier `backend-ai/main.py` modifi√©
- [ ] Backend IA red√©marr√© sur port 8000
- [ ] Test dans l'application r√©ussi

---

## üéØ **AVANTAGES POUR VOTRE PROJET**

### D√©veloppement
- ‚úÖ Testez sans limite
- ‚úÖ Pas de frais
- ‚úÖ Pas besoin de carte bancaire

### Production
- Pour la production, vous pourrez choisir :
  - Ollama (gratuit, priv√©)
  - OpenAI (plus pr√©cis, mais payant)
  - Les deux (OpenAI en priorit√©, Ollama en fallback)

---

## üîÑ **RETOUR √Ä OPENAI (si souhait√© plus tard)**

Pour revenir √† OpenAI :
1. R√©cup√©rez l'ancien code de `main.py` (voir TRIAGE_IA_CLINIQUE.md)
2. Ajoutez des cr√©dits OpenAI
3. Red√©marrez le backend

---

**üéâ Avec Ollama, votre Triage IA est 100% gratuit et illimit√© ! ü§ñüíö**

